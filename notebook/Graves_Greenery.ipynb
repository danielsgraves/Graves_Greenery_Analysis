{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Graves_Greenery_Colab_InMemory",
      "provenance": [],
      "collapsed_sections": [
        "setup-section"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "# ðŸš€ Quick Start â€” Load Data & Enable SQL (No Server)\n",
        "\n",
        "_This collapsible section sets up an **in-memory DuckDB** and the `%%sql` magic so you can run SQL directly in Colab. CSVs from your GitHub repo are loaded into tables named after each file (snake_case)._\n",
        "\n",
        "**What you get:**\n",
        "- One in-memory DuckDB session (no MySQL/SQLite servers)\n",
        "- `%%sql` / `%sql` via `ipython-sql` with pretty table output\n",
        "- Auto-load all CSVs from `data/**` â†’ tables (e.g., `dim_customers`)\n",
        "- A quick verification query you can edit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Install libraries & pull your repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip -q install --upgrade duckdb duckdb-engine \"sqlalchemy>=2.0\" ipython-sql jupysql pandas\n",
        "\n",
        "import os, subprocess\n",
        "REPO_USER = \"danielsgraves\"                 # <-- repo owner\n",
        "REPO_NAME = \"Graves_Greenery_Analysis\"     # <-- repo name\n",
        "REPO_DIR  = f\"/content/{REPO_NAME}\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run(\n",
        "        f\"git clone --depth 1 https://github.com/{REPO_USER}/{REPO_NAME}.git {REPO_DIR}\",\n",
        "        shell=True, check=True\n",
        "    )\n",
        "else:\n",
        "    subprocess.run(f\"git -C {REPO_DIR} pull --ff-only\", shell=True, check=True)\n",
        "\n",
        "print(\"Repo ready at:\", REPO_DIR)\n",
        "print(\"CSV root:\", f\"{REPO_DIR}/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Connect a single in-memory DuckDB session for `%%sql`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%reload_ext sql\n",
        "%config SqlMagic.autopandas = False    # PrettyTable output (set True for DataFrame)\n",
        "%config SqlMagic.feedback = False\n",
        "%sql duckdb:///:memory:\n",
        "print(\"âœ… Connected %sql to in-memory DuckDB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Load all CSVs â†’ tables (names match file stems)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, re, glob\n",
        "from pathlib import Path\n",
        "\n",
        "CSV_GLOB = \"data/**/*.[cC][sS][vV]\"   # case-insensitive .csv\n",
        "INCLUDE_PARENT_PREFIX = False          # True â†’ prefix parent folder: e.g., sales_dim_customers\n",
        "\n",
        "def to_snake(name: str) -> str:\n",
        "    s = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", name).strip(\"_\")\n",
        "    s = re.sub(r\"_+\", \"_\", s)\n",
        "    if s and s[0].isdigit():\n",
        "        s = \"t_\" + s\n",
        "    return s.lower()\n",
        "\n",
        "def table_name_for(csv_path: Path) -> str:\n",
        "    stem = csv_path.stem\n",
        "    if INCLUDE_PARENT_PREFIX and csv_path.parent != csv_path.parent.parent:\n",
        "        return to_snake(csv_path.parent.name + \"_\" + stem)\n",
        "    return to_snake(stem)\n",
        "\n",
        "files = [Path(p) for p in glob.glob(os.path.join(REPO_DIR, CSV_GLOB), recursive=True)]\n",
        "files = [p for p in files if p.is_file()]\n",
        "print(f\"Found {len(files)} CSV(s). Showing first 15 mappingsâ€¦\")\n",
        "for rel, tbl in [(str(p.relative_to(REPO_DIR)), table_name_for(p)) for p in files[:15]]:\n",
        "    print(f\"  {rel}  â†’  {tbl}\")\n",
        "\n",
        "# Create tables via the same %sql connection (no secondary connections)\n",
        "for p in files:\n",
        "    tbl = table_name_for(p)\n",
        "    q = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE \"{tbl}\" AS\n",
        "    SELECT * FROM read_csv_auto('{str(p)}', header=True, sample_size=-1, ignore_errors=True);\n",
        "    \"\"\"\n",
        "    get_ipython().run_cell_magic('sql', '', q)\n",
        "\n",
        "print(\"Loaded tables (first few):\", [table_name_for(p) for p in files[:8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Verify & sample\n",
        "_If your repo has `data/**/dim_customers.csv`, the table will be **`dim_customers`**._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT table_name\n",
        "FROM information_schema.tables\n",
        "WHERE table_schema='main'\n",
        "ORDER BY table_name;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Test query (adjust name if your file is different)\n",
        "SELECT * FROM dim_customers LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solution and Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendations and Next Steps"
      ]
    }
  ]
}
