{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Graves_Greenery_Colab_InMemory",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ¿ Graves Greenery â€” In-Memory SQL (DuckDB + `%%sql`)\n",
        "\n",
        "**Goal:** Type SQL like `%%sql\\nSELECT * FROM dim_customers LIMIT 5;` with **no external DB**. We load CSVs from your GitHub repo into an **in-memory** DuckDB and keep table names matching file stems (snake_case).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip -q install --upgrade duckdb duckdb-engine \"sqlalchemy>=2.0\" ipython-sql jupysql\n",
        "\n",
        "import os, subprocess\n",
        "REPO_USER = \"danielsgraves\"                 # <-- correct owner\n",
        "REPO_NAME = \"Graves_Greenery_Analysis\"\n",
        "REPO_DIR  = f\"/content/{REPO_NAME}\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run(\n",
        "        f\"git clone --depth 1 https://github.com/{REPO_USER}/{REPO_NAME}.git {REPO_DIR}\",\n",
        "        shell=True, check=True\n",
        "    )\n",
        "else:\n",
        "    subprocess.run(f\"git -C {REPO_DIR} pull --ff-only\", shell=True, check=True)\n",
        "\n",
        "print(\"Repo ready at:\", REPO_DIR)\n",
        "print(\"CSV root:\", f\"{REPO_DIR}/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect one in-memory DuckDB session for `%%sql`\n",
        "- No `duckdb.connect()` calls (avoids dual-config conflicts)\n",
        "- One `%sql` connection living entirely in memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%reload_ext sql\n",
        "%sql duckdb:///:memory:\n",
        "print(\"Connected %sql to in-memory DuckDB.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load all CSVs with table names matching file stems\n",
        "- File `data/sales/dim_customers.csv` â†’ table **`dim_customers`**\n",
        "- Non-alphanumeric chars become `_`; numbers get `t_` prefix if needed\n",
        "- Shows a mapping preview before creating tables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, re, glob, html\n",
        "from pathlib import Path\n",
        "\n",
        "CSV_GLOB = \"data/**/*.[cC][sS][vV]\"   # case-insensitive .csv\n",
        "INCLUDE_PARENT_PREFIX = False          # set True to prefix parent folder: e.g., sales_dim_customers\n",
        "\n",
        "def to_snake(name: str) -> str:\n",
        "    s = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", name).strip(\"_\")\n", 
        "    s = re.sub(r\"_+\", \"_\", s)\n",
        "    if s and s[0].isdigit():\n",
        "        s = \"t_\" + s\n",
        "    return s.lower()\n",
        "\n",
        "def table_name_for(csv_path: Path) -> str:\n",
        "    stem = csv_path.stem\n",
        "    if INCLUDE_PARENT_PREFIX and csv_path.parent != csv_path.parent.parent:\n",
        "        return to_snake(csv_path.parent.name + \"_\" + stem)\n",
        "    return to_snake(stem)\n",
        "\n",
        "files = [Path(p) for p in glob.glob(os.path.join(REPO_DIR, CSV_GLOB), recursive=True)]\n",
        "files = [p for p in files if p.is_file()]\n",
        "print(f\"Found {len(files)} CSV(s). Showing first 15 mappingsâ€¦\")\n",
        "preview = [(str(p.relative_to(REPO_DIR)), table_name_for(p)) for p in files[:15]]\n",
        "for rel, tbl in preview:\n",
        "    print(f\"  {rel}  â†’  {tbl}\")\n",
        "\n",
        "# Create tables via the same %sql connection (no secondary connections)\n",
        "for p in files:\n",
        "    tbl = table_name_for(p)\n",
        "    # Safe, minimal quoting: identifier in double-quotes; path in single quotes\n",
        "    q = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE \"{tbl}\" AS\n",
        "    SELECT * FROM read_csv_auto('{str(p)}', header=True, sample_size=-1, ignore_errors=True);\n",
        "    \"\"\"\n",
        "    get_ipython().run_cell_magic('sql', '', q)\n",
        "\n",
        "print(\"Loaded tables (first few):\", [table_name_for(p) for p in files[:8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify tables and run a simple test query\n",
        "- If your repo has `data/**/dim_customers.csv`, the table will be **`dim_customers`**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sql\n",
        "SELECT table_name\n",
        "FROM information_schema.tables\n",
        "WHERE table_schema='main'\n",
        "ORDER BY table_name;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sql\n",
        "-- Test query (adjust name if your file is different)\n",
        "SELECT * FROM dim_customers LIMIT 5;"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
