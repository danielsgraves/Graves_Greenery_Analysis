{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc25cee4",
      "metadata": {},
      "source": [
        "\n",
        "# üåø Graves Greenery ‚Äî Colab SQL Template (DuckDB + `%%sql` + GitHub CSVs)\n",
        "\n",
        "This notebook gives you a **serverless SQL sandbox** for the Graves Greenery project:\n",
        "- Loads CSVs from your **GitHub repo** (public)\n",
        "- Creates tables in a **DuckDB** database (file-backed, persisted in Colab)\n",
        "- Lets you query with **`%%sql` magic** (PrettyTable-style output)\n",
        "- Optional **`%%mysql` magic** to write MySQL syntax and run via DuckDB using SQLGlot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a9f85e",
      "metadata": {},
      "source": [
        "## 1) Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f93a636",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install duckdb ipython-sql pandas duckdb-engine sqlglot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3d0ea3",
      "metadata": {},
      "source": [
        "## 2) Imports & load SQL magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f6d962",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, glob, getpass, subprocess, textwrap, pandas as pd, duckdb\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the ipython-sql extension\n",
        "%load_ext sql"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823ed575",
      "metadata": {},
      "source": [
        "## 3) Configuration ‚Äî repo & DB paths (pre-filled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3301c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === EDIT ONLY IF NEEDED ===\n",
        "GITHUB_USER   = \"danielgraves\"\n",
        "GITHUB_REPO   = \"Graves_Greenery_Analysis\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "SPARSE_PATHS  = [\"data\"]              # not used for public clone, but kept for reference\n",
        "IS_PRIVATE    = False                 # public per your setup\n",
        "\n",
        "# CSVs live in /data within the repo\n",
        "CSV_GLOB      = \"data/**/*.csv\"       # recursive search under data/\n",
        "\n",
        "# Persistent DuckDB database file (saved in Colab environment)\n",
        "DB_PATH       = \"/content/graves_greenery.duckdb\"\n",
        "\n",
        "# Local repo directory in Colab\n",
        "REPO_DIR      = f\"/content/{GITHUB_REPO}\"\n",
        "\n",
        "# Namespacing control: include parent folder names in table names to avoid collisions\n",
        "INCLUDE_PARENT_IN_TABLE = False  # True -> tables like data_orders; False -> orders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a06980",
      "metadata": {},
      "source": [
        "## 4) Start DuckDB (file-backed) and connect `%%sql`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dcfa3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure parent folder exists\n",
        "Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Initialize or connect to the DB\n",
        "_ = duckdb.connect(DB_PATH)\n",
        "_.close()\n",
        "\n",
        "# Connect ipython-sql to the same DB file (shared with %%sql)\n",
        "%sql duckdb:///{DB_PATH}\n",
        "print(\"Connected to DuckDB:\", DB_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b344da8",
      "metadata": {},
      "source": [
        "## 5) Clone (or Pull) your GitHub repo (public)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb1434d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clone_or_pull_repo(user, repo, branch, dest):\n",
        "    if os.path.exists(dest):\n",
        "        print(f\"Repo exists at {dest}. Pulling latest...\")\n",
        "        subprocess.run(f\"git -C {dest} pull --ff-only\", shell=True, check=True)\n",
        "        return\n",
        "\n",
        "    cmd = f\"git clone --depth 1 --branch {branch} https://github.com/{user}/{repo}.git {dest}\"\n",
        "    print(cmd)\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "clone_or_pull_repo(GITHUB_USER, GITHUB_REPO, GITHUB_BRANCH, REPO_DIR)\n",
        "print(\"Repo ready at:\", REPO_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "868b45e1",
      "metadata": {},
      "source": [
        "## 6) Load CSVs into DuckDB tables (auto schema inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464bb0d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "con = duckdb.connect(DB_PATH)\n",
        "\n",
        "def slugify_table_name(path, include_parent=False):\n",
        "    path = Path(path)\n",
        "    stem = path.stem.lower()\n",
        "    stem = re.sub(r\"[^a-z0-9_]+\", \"_\", stem)\n",
        "    stem = re.sub(r\"_+\", \"_\", stem).strip(\"_\")\n",
        "    if include_parent and path.parent != path.parent.parent:\n",
        "        parent = re.sub(r\"[^a-z0-9_]+\", \"_\", path.parent.name.lower()).strip(\"_\")\n",
        "        name = f\"{parent}_{stem}\"\n",
        "    else:\n",
        "        name = stem\n",
        "    if re.match(r\"^\\\\d\", name):\n",
        "        name = \"t_\" + name\n",
        "    return name\n",
        "\n",
        "def load_csvs_as_tables(repo_dir, csv_glob, include_parent=False):\n",
        "    csvs = glob.glob(os.path.join(repo_dir, csv_glob), recursive=True)\n",
        "    loaded = []\n",
        "    for f in csvs:\n",
        "        tbl = slugify_table_name(f, include_parent=include_parent)\n",
        "        # Use read_csv_auto for robust type inference; ignore bad rows to keep loading resilient\n",
        "        con.execute(f\"\"\"\n",
        "            CREATE OR REPLACE TABLE \"{tbl}\" AS\n",
        "            SELECT * FROM read_csv_auto(?, header=True, sample_size=-1, ignore_errors=True);\n",
        "        \"\"\", [f])\n",
        "        loaded.append((tbl, f))\n",
        "    return loaded\n",
        "\n",
        "loaded = load_csvs_as_tables(REPO_DIR, CSV_GLOB, INCLUDE_PARENT_IN_TABLE)\n",
        "print(f\"Loaded {len(loaded)} CSVs into DuckDB tables.\")\n",
        "loaded[:10]  # preview first 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0a5aac",
      "metadata": {},
      "source": [
        "## 7) Verify: list available tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b8fcd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT table_name\n",
        "FROM duckdb_tables()\n",
        "WHERE NOT internal\n",
        "ORDER BY table_name;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a698afd6",
      "metadata": {},
      "source": [
        "## 8) (Optional) `%%mysql` magic ‚Äî write MySQL, run on DuckDB via SQLGlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb90f7bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sqlglot import transpile\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def mysql(line, cell):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "    %%mysql\n",
        "    SELECT DATE_FORMAT(CURRENT_DATE(), '%Y-%m-%d') AS today;\n",
        "    \"\"\"\n",
        "    # Translate MySQL -> DuckDB\n",
        "    [duckdb_sql] = transpile(cell, read=\"mysql\", write=\"duckdb\")\n",
        "    # Minor compatibility tweaks (extend as needed)\n",
        "    duckdb_sql = (duckdb_sql\n",
        "                  .replace(\"IFNULL\", \"COALESCE\")\n",
        "                  .replace(\"NOW()\", \"CURRENT_TIMESTAMP\"))\n",
        "    print(\"Translated to DuckDB SQL:\\n\", duckdb_sql, \"\\n\", flush=True)\n",
        "    # Execute through ipython-sql so we get PrettyTable rendering\n",
        "    return get_ipython().run_cell_magic('sql', '', duckdb_sql)\n",
        "\n",
        "print(\"Custom %%mysql magic is ready. Try it in the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78b065e",
      "metadata": {},
      "source": [
        "## 9) Example queries ‚Äî Graves Greenery Starter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5099e52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è Edit table names below if your CSV filenames differ.\n",
        "# Common expected tables: plants, categories, customers, orders, order_items\n",
        "\n",
        "# Top sellers by revenue\n",
        "_ = get_ipython().run_cell_magic('sql', '', textwrap.dedent(\"\"\"\n",
        "SELECT p.name AS plant,\n",
        "       SUM(oi.quantity)                        AS units_sold,\n",
        "       SUM(oi.quantity * oi.unit_price)        AS revenue\n",
        "FROM order_items oi\n",
        "JOIN plants p ON p.plant_id = oi.plant_id\n",
        "GROUP BY 1\n",
        "ORDER BY revenue DESC\n",
        "LIMIT 10;\n",
        "\"\"\"))\n",
        "\n",
        "# Repeat purchase rate (simple version)\n",
        "_ = get_ipython().run_cell_magic('sql', '', textwrap.dedent(\"\"\"\n",
        "WITH per_customer AS (\n",
        "  SELECT customer_id, COUNT(DISTINCT order_id) AS orders_n\n",
        "  FROM orders\n",
        "  GROUP BY customer_id\n",
        ")\n",
        "SELECT\n",
        "  SUM(CASE WHEN orders_n >= 2 THEN 1 ELSE 0 END) * 1.0 / COUNT(*) AS repeat_rate\n",
        "FROM per_customer;\n",
        "\"\"\"))\n",
        "\n",
        "# Monthly revenue trend\n",
        "_ = get_ipython().run_cell_magic('sql', '', textwrap.dedent(\"\"\"\n",
        "SELECT\n",
        "  DATE_TRUNC('month', order_date)::DATE AS month,\n",
        "  SUM(order_total) AS revenue\n",
        "FROM orders\n",
        "GROUP BY 1\n",
        "ORDER BY 1;\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548c06a4",
      "metadata": {},
      "source": [
        "## 10) Refresh data (git pull + reload CSVs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b6ca6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull latest changes (re-run this cell when repo updates)\n",
        "subprocess.run(f\"git -C {REPO_DIR} pull --ff-only\", shell=True, check=True)\n",
        "\n",
        "# Rebuild tables\n",
        "loaded = load_csvs_as_tables(REPO_DIR, CSV_GLOB, INCLUDE_PARENT_IN_TABLE)\n",
        "print(f\"Reloaded {len(loaded)} CSVs into DuckDB tables.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e830aecf",
      "metadata": {},
      "source": [
        "## 11) Snapshot / export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3a2a83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: The DuckDB file at DB_PATH is your snapshot already.\n",
        "print(\"DuckDB snapshot at:\", DB_PATH)\n",
        "\n",
        "# Option B: Export a query to CSV (example)\n",
        "export_path = \"/content/top_plants.csv\"\n",
        "_ = get_ipython().run_cell_magic('sql', '', textwrap.dedent(f\"\"\"\n",
        "COPY (\n",
        "  SELECT p.name AS plant,\n",
        "         SUM(oi.quantity)                 AS units_sold,\n",
        "         SUM(oi.quantity * oi.unit_price) AS revenue\n",
        "  FROM order_items oi\n",
        "  JOIN plants p ON p.plant_id = oi.plant_id\n",
        "  GROUP BY 1\n",
        "  ORDER BY revenue DESC\n",
        "  LIMIT 50\n",
        ") TO '{export_path}' WITH (HEADER, DELIMITER ',');\n",
        "\"\"\"))\n",
        "print(\"Exported CSV:\", export_path)\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
